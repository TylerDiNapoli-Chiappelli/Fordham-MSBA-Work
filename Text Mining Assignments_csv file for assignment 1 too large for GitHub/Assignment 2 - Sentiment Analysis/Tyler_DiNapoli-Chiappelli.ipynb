{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Needed Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data set, name the columns, and separate the \"labels\" from the reviews (implicitly dropping the unneeded column)\n",
    "# I would normally convert everything to lowercase, but the file seems to come all in lowercase\n",
    "movies = pd.read_csv('MovieReview-Sample.csv', names = ['Review_Number','Review','Positive_or_Negative'])\n",
    "\n",
    "actual_classifications = movies['Positive_or_Negative']\n",
    "reviews = movies['Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Four Performance Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1a: Bing Liu's Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the \"Positive and Negative Words Counter\" Function (stolen directly from Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_positive_and_negative_words(data, positive_words_dictionary, negative_words_dictionary):\n",
    "# count of positive and negative words that appeared in each review\n",
    "# net count which is calculated by positive count subtracting negative count. \n",
    "    positive_count = []\n",
    "    negative_count = []\n",
    "    net_assessment = []\n",
    "\n",
    "    for nrow in range(0,len(data)):\n",
    "        text = data[nrow]\n",
    "        \n",
    "        qa = 0\n",
    "        qb = 0\n",
    "\n",
    "        for word in positive_words_dictionary:\n",
    "            if (word in text):\n",
    "                qa += 1\n",
    "\n",
    "        for word in negative_words_dictionary:\n",
    "            if (word in text):\n",
    "                qb += 1\n",
    "\n",
    "        qc = qa - qb\n",
    "\n",
    "        positive_count.append(qa)\n",
    "        negative_count.append(qb)\n",
    "        net_assessment.append(qc)\n",
    "\n",
    "    #Make a new data frame of the results\n",
    "    results = {'Review': reviews, 'Postive_Words_Count': positive_count, 'Negative_Words_Count': negative_count, \n",
    "                    'Overall_Sentiment': net_assessment}\n",
    "    \n",
    "    dataframe = pd.DataFrame(results)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Bing Liu's Lexicon to the movie reviews data and compare with the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Bing Liu's Lexicon (importing it was done with all the rest of the Python modules)\n",
    "#nltk.download(\"opinion_lexicon\")\n",
    "\n",
    "#Create our separate sets of positive and negative words from Bing Liu's Lexicon\n",
    "positive_list_BL=set(opinion_lexicon.positive())\n",
    "negative_list_BL=set(opinion_lexicon.negative())\n",
    "\n",
    "#Using Bing Liu's Lexicon and our \"counter\" function, generate the assessments of overall positiveity and negativity\n",
    "#And, make a new data frame of the results\n",
    "Bing_Liu_Lexicon_dataframe = count_positive_and_negative_words(reviews, positive_list_BL, negative_list_BL)\n",
    "\n",
    "#Add a new column to the dataframe, converting the \"Overall_Sentiment\" into 1 or 0.\n",
    "#Note, I will consider a net score of 0 to be in the negative bucket; thus it will get coded as 0. I consider a movie which\n",
    "#generates a totally neutral sentiment to be a flop.\n",
    "Bing_Liu_model_classifications = []\n",
    "for i in Bing_Liu_Lexicon_dataframe['Overall_Sentiment']:\n",
    "    if i > 0:\n",
    "        Bing_Liu_model_classifications.append(1)\n",
    "    else:\n",
    "        Bing_Liu_model_classifications.append(0)\n",
    "        \n",
    "Bing_Liu_Lexicon_dataframe['Bing_Liu_Model_Classifications'] = Bing_Liu_model_classifications\n",
    "        \n",
    "#Add the true labels back to the dataframe so we can see how Bing Liu's Lexicon did\n",
    "Bing_Liu_Lexicon_dataframe['Actual_Classifications'] = actual_classifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the performance of the Bing Liu Lexicon Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the predicted one and zeros and the actual ones and zeros\n",
    "#confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#Note from sci-k itlearn.org:\n",
    "#Thus in binary classification, the count of true negatives is M[0,0], false negatives is M[1,0], \n",
    "    #true positives is M[1,1], and false positives is M[0,1] .\n",
    "Bing_Liu_confusion_matrix = confusion_matrix(Bing_Liu_Lexicon_dataframe['Actual_Classifications'], Bing_Liu_Lexicon_dataframe['Bing_Liu_Model_Classifications'])\n",
    "\n",
    "Bing_Liu_true_positive = Bing_Liu_confusion_matrix[1,1]\n",
    "Bing_Liu_true_negative = Bing_Liu_confusion_matrix[0,0]\n",
    "Bing_Liu_false_positive = Bing_Liu_confusion_matrix[0,1]\n",
    "Bing_Liu_false_negative = Bing_Liu_confusion_matrix[1,0]\n",
    "\n",
    "#Calculate positive precision\n",
    "Bing_Liu_positive_precision = Bing_Liu_true_positive / (Bing_Liu_true_positive + Bing_Liu_false_positive)\n",
    "\n",
    "#Calculate negative precision\n",
    "Bing_Liu_negative_precision = Bing_Liu_true_negative / (Bing_Liu_true_negative + Bing_Liu_false_negative)\n",
    "\n",
    "#Calculate average precision\n",
    "Bing_Liu_average_precision = (Bing_Liu_positive_precision + Bing_Liu_negative_precision) / 2\n",
    "\n",
    "#Calculate positive recall\n",
    "Bing_Liu_positive_recall = Bing_Liu_true_positive / (Bing_Liu_true_positive + Bing_Liu_false_negative)\n",
    "\n",
    "#Calculate negative recall\n",
    "Bing_Liu_negative_recall = Bing_Liu_true_negative / (Bing_Liu_true_negative + Bing_Liu_false_positive)\n",
    "\n",
    "#Calculate average recall\n",
    "Bing_Liu_average_recall = (Bing_Liu_positive_recall + Bing_Liu_negative_recall) / 2\n",
    "\n",
    "#Calculate average F score\n",
    "Bing_Liu_average_F_score = (2 * Bing_Liu_average_precision * Bing_Liu_average_recall) / (Bing_Liu_average_precision + Bing_Liu_average_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1b: Loughran-McDonald (LM) Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the LM Dictionary from the two lab files, \"negative-words-LM\" and \"positive-words-LM\" (stolen directly from Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_local_dictionary(file):\n",
    "    # create dictionary list\n",
    "    LM_dictionary = []\n",
    "    with open(file, \"r\") as f: \n",
    "        for line in f:\n",
    "            t = line.strip().lower()\n",
    "            LM_dictionary.append(t)\n",
    "    return LM_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LM Dictionary to the movie reviews data and compare with the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our separate sets of positive and negative words from the LM files\n",
    "positive_list_LM = read_local_dictionary('positive-words-LM.txt')\n",
    "negative_list_LM = read_local_dictionary('negative-words-LM.txt')\n",
    "\n",
    "#Using the LM Dictionary and our \"counter\" function, generate the assessments of overall positivity and negativity\n",
    "#And, make a new data frame of the results\n",
    "LM_dataframe = count_positive_and_negative_words(reviews, positive_list_LM, negative_list_LM)\n",
    "\n",
    "#Add a new column to the dataframe, converting the \"Overall_Sentiment\" into 1 or 0.\n",
    "#Note, I will consider a net score of 0 to be in the negative bucket; thus it will get coded as 0. I consider a movie which\n",
    "#generates a totally neutral sentiment to be a flop.\n",
    "LM_model_classifications = []\n",
    "for i in LM_dataframe['Overall_Sentiment']:\n",
    "    if i > 0:\n",
    "        LM_model_classifications.append(1)\n",
    "    else:\n",
    "        LM_model_classifications.append(0)\n",
    "        \n",
    "LM_dataframe['LM_Model_Classifications'] = LM_model_classifications\n",
    "        \n",
    "#Add the true labels back to the dataframe so we can see how the LM Dictionary did\n",
    "LM_dataframe['Actual_Classifications'] = actual_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the performance of the LM Dictionary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the predicted one and zeros and the actual ones and zeros\n",
    "#confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#Note from sci-k itlearn.org:\n",
    "#Thus in binary classification, the count of true negatives is M[0,0], false negatives is M[1,0], \n",
    "    #true positives is M[1,1], and false positives is M[0,1] .\n",
    "LM_confusion_matrix = confusion_matrix(LM_dataframe['Actual_Classifications'], LM_dataframe['LM_Model_Classifications'])\n",
    "\n",
    "LM_true_positive = LM_confusion_matrix[1,1]\n",
    "LM_true_negative = LM_confusion_matrix[0,0]\n",
    "LM_false_positive = LM_confusion_matrix[0,1]\n",
    "LM_false_negative = LM_confusion_matrix[1,0]\n",
    "\n",
    "#Calculate positive precision\n",
    "LM_positive_precision = LM_true_positive / (LM_true_positive + LM_false_positive)\n",
    "\n",
    "#Calculate negative precision\n",
    "LM_negative_precision = LM_true_negative / (LM_true_negative + LM_false_negative)\n",
    "\n",
    "#Calculate average precision\n",
    "LM_average_precision = (LM_positive_precision + LM_negative_precision) / 2\n",
    "\n",
    "#Calculate positive recall\n",
    "LM_positive_recall = LM_true_positive / (LM_true_positive + LM_false_negative)\n",
    "\n",
    "#Calculate negative recall\n",
    "LM_negative_recall = LM_true_negative / (LM_true_negative + LM_false_positive)\n",
    "\n",
    "#Calculate average recall\n",
    "LM_average_recall = (LM_positive_recall + LM_negative_recall) / 2\n",
    "\n",
    "#Calculate average F score\n",
    "LM_average_F_score = (2 * LM_average_precision * LM_average_recall) / (LM_average_precision + LM_average_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1c: Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Textblob Dataframe (stolen directly from Lab) and compare with true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_dataframe = pd.DataFrame({'Review': reviews})\n",
    "textblob_dataframe[\"Textblob_Score\"] = textblob_dataframe[\"Review\"].map(lambda x:TextBlob(x).sentiment.polarity)\n",
    "\n",
    "#Note:  TextBlob’s output for a polarity task is a float within the range [-1.0, 1.0] where -1.0 is a \n",
    "        #negative polarity and 1.0 is positive. This score can also be equal to 0, which stands for a neutral \n",
    "        #evaluation of a statement as it doesn’t contain any words from the training set.\n",
    "        \n",
    "#Add a new column to the dataframe, converting the \"Overall_Sentiment\" into 1 or 0.\n",
    "#Note, I will consider a net score of 0 to be in the negative bucket; thus it will get coded as 0. I consider a movie which\n",
    "#generates a totally neutral sentiment to be a flop.\n",
    "textblob_dataframe_classifications = []\n",
    "for i in textblob_dataframe['Textblob_Score']:\n",
    "    if i > 0:\n",
    "        textblob_dataframe_classifications.append(1)\n",
    "    else:\n",
    "        textblob_dataframe_classifications.append(0)\n",
    "        \n",
    "textblob_dataframe['Textblob_Model_Classifications'] = textblob_dataframe_classifications\n",
    "        \n",
    "#Add the true labels back to the dataframe so we can see how Textblob did\n",
    "textblob_dataframe['Actual_Classifications'] = actual_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the performance of the Textblob Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the predicted one and zeros and the actual ones and zeros\n",
    "#confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#Note from sci-k itlearn.org:\n",
    "#Thus in binary classification, the count of true negatives is M[0,0], false negatives is M[1,0], \n",
    "    #true positives is M[1,1], and false positives is M[0,1] .\n",
    "textblob_confusion_matrix = confusion_matrix(textblob_dataframe['Actual_Classifications'], textblob_dataframe['Textblob_Model_Classifications'])\n",
    "\n",
    "textblob_true_positive = textblob_confusion_matrix[1,1]\n",
    "textblob_true_negative = textblob_confusion_matrix[0,0]\n",
    "textblob_false_positive = textblob_confusion_matrix[0,1]\n",
    "textblob_false_negative = textblob_confusion_matrix[1,0]\n",
    "\n",
    "#Calculate positive precision\n",
    "textblob_positive_precision = textblob_true_positive / (textblob_true_positive + textblob_false_positive)\n",
    "\n",
    "#Calculate negative precision\n",
    "textblob_negative_precision = textblob_true_negative / (textblob_true_negative + textblob_false_negative)\n",
    "\n",
    "#Calculate average precision\n",
    "textblob_average_precision = (textblob_positive_precision + textblob_negative_precision) / 2\n",
    "\n",
    "#Calculate positive recall\n",
    "textblob_positive_recall = textblob_true_positive / (textblob_true_positive + textblob_false_negative)\n",
    "\n",
    "#Calculate negative recall\n",
    "textblob_negative_recall = textblob_true_negative / (textblob_true_negative + textblob_false_positive)\n",
    "\n",
    "#Calculate average recall\n",
    "textblob_average_recall = (textblob_positive_recall + textblob_negative_recall) / 2\n",
    "\n",
    "#Calculate average F score\n",
    "textblob_average_F_score = (2 * textblob_average_precision * textblob_average_recall) / (textblob_average_precision + textblob_average_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1d: VADER - from NLTK (stolen mostly from Lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make VADER dataframe and compare with true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup VADER scores output (in the form of a Python dictionary)\n",
    "scores = [SentimentIntensityAnalyzer().polarity_scores(sentence) for sentence in reviews]\n",
    "\n",
    "#Separate the sentiments into the respective buckets, per the above dictionary output\n",
    "negative_sentiment = [i[\"neg\"] for i in scores]\n",
    "neutral_sentiment = [i[\"neu\"] for i in scores]\n",
    "positive_sentiment = [i[\"pos\"] for i in scores]\n",
    "compound_sentiment = [i[\"compound\"] for i in scores]\n",
    "\n",
    "#Create VADER Dataframe with \"compound sentiment\" score\n",
    "VADER_dataframe = pd.DataFrame({'Review': reviews})\n",
    "VADER_dataframe['VADER_Compound_Score'] = compound_sentiment\n",
    "\n",
    "#Add a new column to the dataframe, converting the \"Compound_Score\" into 1 or 0.\n",
    "#Note, I will consider neutral scores to be in the negative bucket; thus it will get coded as 0. I consider a movie which\n",
    "#generates a totally neutral sentiment to be a flop.\n",
    "\n",
    "#Another note: per the guidance in the Lab, positive scores will be where the Compound_Score >= .05\n",
    "VADER_dataframe_classifications = []\n",
    "for i in VADER_dataframe['VADER_Compound_Score']:\n",
    "    if i >= .05:\n",
    "        VADER_dataframe_classifications.append(1)\n",
    "    else:\n",
    "        VADER_dataframe_classifications.append(0)\n",
    "        \n",
    "VADER_dataframe['VADER_Model_Classifications'] = VADER_dataframe_classifications\n",
    "\n",
    "#Add true labels for comparison with VADER model\n",
    "VADER_dataframe['Actual_Classifications'] = actual_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the performance of the VADER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the predicted one and zeros and the actual ones and zeros\n",
    "#confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#Note from sci-k itlearn.org:\n",
    "#Thus in binary classification, the count of true negatives is M[0,0], false negatives is M[1,0], \n",
    "    #true positives is M[1,1], and false positives is M[0,1] .\n",
    "VADER_confusion_matrix = confusion_matrix(VADER_dataframe['Actual_Classifications'], VADER_dataframe['VADER_Model_Classifications'])\n",
    "\n",
    "VADER_true_positive = VADER_confusion_matrix[1,1]\n",
    "VADER_true_negative = VADER_confusion_matrix[0,0]\n",
    "VADER_false_positive = VADER_confusion_matrix[0,1]\n",
    "VADER_false_negative = VADER_confusion_matrix[1,0]\n",
    "\n",
    "#Calculate positive precision\n",
    "VADER_positive_precision = VADER_true_positive / (VADER_true_positive + VADER_false_positive)\n",
    "\n",
    "#Calculate negative precision\n",
    "VADER_negative_precision = VADER_true_negative / (VADER_true_negative + VADER_false_negative)\n",
    "\n",
    "#Calculate average precision\n",
    "VADER_average_precision =  (VADER_positive_precision + VADER_negative_precision) / 2\n",
    "\n",
    "#Calculate positive recall\n",
    "VADER_positive_recall = VADER_true_positive / (VADER_true_positive + VADER_false_negative)\n",
    "\n",
    "#Calculate negative recall\n",
    "VADER_negative_recall = VADER_true_negative / (VADER_true_negative + VADER_false_positive)\n",
    "\n",
    "#Calculate average recall\n",
    "VADER_average_recall = (VADER_positive_recall + VADER_negative_recall) / 2\n",
    "\n",
    "#Calculate average F score\n",
    "VADER_average_F_score = (2 * VADER_average_precision * VADER_average_recall) / (VADER_average_precision + VADER_average_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1e: Make Performance Comparison Table of the Four Previous Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Bing Liu Dictionary', 'LM Dictionary', 'Textblob', 'VADER']\n",
    "precision_list = [Bing_Liu_average_precision, LM_average_precision, textblob_average_precision, VADER_average_precision]\n",
    "recall_list = [Bing_Liu_average_recall, LM_average_recall, textblob_average_recall, VADER_average_recall]\n",
    "F_score_list = [Bing_Liu_average_F_score, LM_average_F_score, textblob_average_F_score, VADER_average_F_score]\n",
    "\n",
    "data = {'Model Name': model_names, 'Average Precision': precision_list, 'Average Recall': recall_list, \n",
    "                    'Average F-Score': F_score_list}\n",
    "\n",
    "model_comparisons = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "      <th>Average F-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bing Liu Dictionary</td>\n",
       "      <td>0.661706</td>\n",
       "      <td>0.5495</td>\n",
       "      <td>0.600406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LM Dictionary</td>\n",
       "      <td>0.612191</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>0.576924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Textblob</td>\n",
       "      <td>0.667701</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.630653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VADER</td>\n",
       "      <td>0.607468</td>\n",
       "      <td>0.6025</td>\n",
       "      <td>0.604974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model Name  Average Precision  Average Recall  Average F-Score\n",
       "0  Bing Liu Dictionary           0.661706          0.5495         0.600406\n",
       "1        LM Dictionary           0.612191          0.5455         0.576924\n",
       "2             Textblob           0.667701          0.5975         0.630653\n",
       "3                VADER           0.607468          0.6025         0.604974"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Ensembling to Increase Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Assemble the dataframe with the three best (as defined by F-score, since precision and recall are of approximately equal importance in movie reviews classification) models' calculations and normalize the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focusing on F-scores leads me to drop the LM Dictionary\n",
    "side_by_side_model_calculations = pd.DataFrame({'Reviews': reviews, 'Bing_Liu_Model_Final_Sentiment': Bing_Liu_Lexicon_dataframe['Overall_Sentiment'],\n",
    "                                           'Textblob_Model_Final_Sentiment': textblob_dataframe['Textblob_Score'],\n",
    "                                           'VADER_Model_Final_Sentiment': VADER_dataframe['VADER_Compound_Score']})\n",
    "\n",
    "#Normalize values so everything is on the same scale\n",
    "standardized_calculations = side_by_side_model_calculations.copy()\n",
    "calculations_list = ['Bing_Liu_Model_Final_Sentiment', 'Textblob_Model_Final_Sentiment', 'VADER_Model_Final_Sentiment']\n",
    "\n",
    "original_column_values = side_by_side_model_calculations[calculations_list]\n",
    "sample_mean = side_by_side_model_calculations[calculations_list].mean()\n",
    "sample_stddev = side_by_side_model_calculations[calculations_list].std()\n",
    "\n",
    "standardized_calculations[calculations_list] = ((original_column_values - sample_mean)/sample_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Bing_Liu_Model_Final_Sentiment</th>\n",
       "      <th>Textblob_Model_Final_Sentiment</th>\n",
       "      <th>VADER_Model_Final_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "      <td>-0.703362</td>\n",
       "      <td>-1.544088</td>\n",
       "      <td>-1.401508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "      <td>0.114322</td>\n",
       "      <td>0.056327</td>\n",
       "      <td>0.688920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you've got mail works alot better than it dese...</td>\n",
       "      <td>1.749691</td>\n",
       "      <td>-0.012745</td>\n",
       "      <td>0.906751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaws   is a rare film that grabs your attent...</td>\n",
       "      <td>-0.805572</td>\n",
       "      <td>-0.176215</td>\n",
       "      <td>0.848942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moviemaking is a lot like being the general ma...</td>\n",
       "      <td>0.625375</td>\n",
       "      <td>-0.301433</td>\n",
       "      <td>0.897310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  \\\n",
       "0  films adapted from comic books have had plenty...   \n",
       "1  every now and then a movie comes along from a ...   \n",
       "2  you've got mail works alot better than it dese...   \n",
       "3    jaws   is a rare film that grabs your attent...   \n",
       "4  moviemaking is a lot like being the general ma...   \n",
       "\n",
       "   Bing_Liu_Model_Final_Sentiment  Textblob_Model_Final_Sentiment  \\\n",
       "0                       -0.703362                       -1.544088   \n",
       "1                        0.114322                        0.056327   \n",
       "2                        1.749691                       -0.012745   \n",
       "3                       -0.805572                       -0.176215   \n",
       "4                        0.625375                       -0.301433   \n",
       "\n",
       "   VADER_Model_Final_Sentiment  \n",
       "0                    -1.401508  \n",
       "1                     0.688920  \n",
       "2                     0.906751  \n",
       "3                     0.848942  \n",
       "4                     0.897310  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_calculations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create rules for a new classification column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the rule and make a new vector based off of that rule\n",
    "ensembled_classifications = []\n",
    "\n",
    "for i in range(len(standardized_calculations['Reviews'])):\n",
    "    if (standardized_calculations['Bing_Liu_Model_Final_Sentiment'][i] +\n",
    "        standardized_calculations['Textblob_Model_Final_Sentiment'][i] + \n",
    "        standardized_calculations['VADER_Model_Final_Sentiment'][i]) > 0:\n",
    "        \n",
    "        ensembled_classifications.append(1)\n",
    "        \n",
    "    else:\n",
    "        ensembled_classifications.append(0)\n",
    "        \n",
    "#Stick the new vector on the existing dataframe\n",
    "standardized_calculations['Ensembled_Model_Classifications'] = ensembled_classifications\n",
    "\n",
    "#Finally, add the \"actual_classifications\" vector\n",
    "standardized_calculations['Actual_Classifications'] = actual_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute Performance of Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the predicted one and zeros and the actual ones and zeros\n",
    "#confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#Note from sci-k itlearn.org:\n",
    "#Thus in binary classification, the count of true negatives is M[0,0], false negatives is M[1,0], \n",
    "    #true positives is M[1,1], and false positives is M[0,1] .\n",
    "ensemble_confusion_matrix = confusion_matrix(standardized_calculations['Actual_Classifications'], standardized_calculations['Ensembled_Model_Classifications'])\n",
    "\n",
    "ensemble_true_positive = ensemble_confusion_matrix[1,1]\n",
    "ensemble_true_negative = ensemble_confusion_matrix[0,0]\n",
    "ensemble_false_positive = ensemble_confusion_matrix[0,1]\n",
    "ensemble_false_negative = ensemble_confusion_matrix[1,0]\n",
    "\n",
    "#Calculate positive precision\n",
    "ensemble_positive_precision = ensemble_true_positive / (ensemble_true_positive + ensemble_false_positive)\n",
    "\n",
    "#Calculate negative precision\n",
    "ensemble_negative_precision = ensemble_true_negative / (ensemble_true_negative + ensemble_false_negative)\n",
    "\n",
    "#Calculate average precision\n",
    "ensemble_average_precision =  (ensemble_positive_precision + ensemble_negative_precision) / 2\n",
    "\n",
    "#Calculate positive recall\n",
    "ensemble_positive_recall = ensemble_true_positive / (ensemble_true_positive + ensemble_false_negative)\n",
    "\n",
    "#Calculate negative recall\n",
    "ensemble_negative_recall = ensemble_true_negative / (ensemble_true_negative + ensemble_false_positive)\n",
    "\n",
    "#Calculate average recall\n",
    "ensemble_average_recall = (ensemble_positive_recall + ensemble_negative_recall) / 2\n",
    "\n",
    "#Calculate average F score\n",
    "ensemble_average_F_score = (2 * ensemble_average_precision * ensemble_average_recall) / (ensemble_average_precision + ensemble_average_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_2 = ['Bing Liu Dictionary', 'Textblob', 'VADER', 'Ensemble Model']\n",
    "precision_list_2 = [Bing_Liu_average_precision, textblob_average_precision, VADER_average_precision, ensemble_average_precision]\n",
    "recall_list_2 = [Bing_Liu_average_recall, textblob_average_recall, VADER_average_recall, ensemble_average_recall]\n",
    "F_score_list_2 = [Bing_Liu_average_F_score, textblob_average_F_score, VADER_average_F_score, ensemble_average_F_score]\n",
    "\n",
    "data_2 = {'Model Name': model_names_2, 'Average Precision': precision_list_2, 'Average Recall': recall_list_2, \n",
    "                    'Average F-Score': F_score_list_2}\n",
    "\n",
    "model_comparisons_2 = pd.DataFrame(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "      <th>Average F-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bing Liu Dictionary</td>\n",
       "      <td>0.661706</td>\n",
       "      <td>0.5495</td>\n",
       "      <td>0.600406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Textblob</td>\n",
       "      <td>0.667701</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.630653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VADER</td>\n",
       "      <td>0.607468</td>\n",
       "      <td>0.6025</td>\n",
       "      <td>0.604974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensemble Model</td>\n",
       "      <td>0.651220</td>\n",
       "      <td>0.6505</td>\n",
       "      <td>0.650860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model Name  Average Precision  Average Recall  Average F-Score\n",
       "0  Bing Liu Dictionary           0.661706          0.5495         0.600406\n",
       "1             Textblob           0.667701          0.5975         0.630653\n",
       "2                VADER           0.607468          0.6025         0.604974\n",
       "3       Ensemble Model           0.651220          0.6505         0.650860"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_comparisons_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make table of percentage improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe without Ensemble Model in it\n",
    "model_comparisons_3 = model_comparisons_2[model_comparisons_2['Model Name'] != 'Ensemble Model']\n",
    "\n",
    "#Calculate percentage improvement for each value in each column\n",
    "precision_improvement = []\n",
    "for i in model_comparisons_3['Average Precision']:\n",
    "    precision_improvement.append(((ensemble_average_precision - i) / ensemble_average_precision)*100)\n",
    "\n",
    "recall_improvement = []\n",
    "for i in model_comparisons_3['Average Recall']:\n",
    "    recall_improvement.append(((ensemble_average_recall - i) / ensemble_average_recall)*100)\n",
    "    \n",
    "F_score_improvement = []\n",
    "for i in model_comparisons_3['Average F-Score']:\n",
    "    F_score_improvement.append(((ensemble_average_F_score - i) / ensemble_average_F_score)*100)\n",
    "    \n",
    "#Make new dataframe for the percentage improvements\n",
    "data_3 = {'Inferior Model Name': model_comparisons_3['Model Name'], \n",
    "          'Precision Improvement of Ensemble Over Inferior Model (%)': precision_improvement,\n",
    "         'Recall Improvement of Ensemble Over Inferior Model (%)': recall_improvement, \n",
    "          'F-Score Improvement of Ensemble Over Inferior Model (%)': F_score_improvement}\n",
    "\n",
    "percentage_improvements = pd.DataFrame(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inferior Model Name</th>\n",
       "      <th>Precision Improvement of Ensemble Over Inferior Model (%)</th>\n",
       "      <th>Recall Improvement of Ensemble Over Inferior Model (%)</th>\n",
       "      <th>F-Score Improvement of Ensemble Over Inferior Model (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bing Liu Dictionary</td>\n",
       "      <td>-1.610222</td>\n",
       "      <td>15.526518</td>\n",
       "      <td>7.751921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Textblob</td>\n",
       "      <td>-2.530834</td>\n",
       "      <td>8.147579</td>\n",
       "      <td>3.104625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VADER</td>\n",
       "      <td>6.718508</td>\n",
       "      <td>7.378939</td>\n",
       "      <td>7.050079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Inferior Model Name  \\\n",
       "0  Bing Liu Dictionary   \n",
       "1             Textblob   \n",
       "2                VADER   \n",
       "\n",
       "   Precision Improvement of Ensemble Over Inferior Model (%)  \\\n",
       "0                                          -1.610222           \n",
       "1                                          -2.530834           \n",
       "2                                           6.718508           \n",
       "\n",
       "   Recall Improvement of Ensemble Over Inferior Model (%)  \\\n",
       "0                                          15.526518        \n",
       "1                                           8.147579        \n",
       "2                                           7.378939        \n",
       "\n",
       "   F-Score Improvement of Ensemble Over Inferior Model (%)  \n",
       "0                                           7.751921        \n",
       "1                                           3.104625        \n",
       "2                                           7.050079        "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
