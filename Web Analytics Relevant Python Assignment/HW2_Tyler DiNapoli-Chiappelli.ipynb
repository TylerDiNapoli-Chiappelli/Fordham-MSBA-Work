{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #1: Using Tweepy to Crawl Twitter\n",
    "\n",
    "Focal Product: Gibson guitars\n",
    "\n",
    "Competing Brands with the \"focal product\": Fender, Paul Reed Smith\n",
    "\n",
    "Product Features of Interest: finish, pickups, wood type\n",
    "\n",
    "Selected Hashtags: #guitarfinish, #guitarpickups, #guitarwood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 rows of data crawled from Twitter related to Gibson\n",
      "100 rows of data crawled from Twitter related to Fender\n",
      "100 rows of data crawled from Twitter related to Paul Reed Smith\n"
     ]
    }
   ],
   "source": [
    "#Please note the following code was copy-and-pasted from the in-class lab on Tweepy. Changes were made to the \"try\" portion of\n",
    "#the code. After working with Mandi, we discovered that a few lines of code within that loop were not playing nice with my\n",
    "#computer itself. Root cause unclear, but we did work around.\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#set (‘API Key’，‘API Secret Key')\n",
    "auth = tweepy.auth.OAuthHandler('2h6cgJQPfpuA77NToffVnUGvX', '2D7CYmr3BLnsdG4lyHp3hlwikGsKbAD6ylIBFZAoTQB2KY9gBn')\n",
    "# set ('Access token', 'Access token secret')\n",
    "auth.set_access_token('1311452759036657664-baOnxR2RLDTrJXAC1L105oWr4HfAsT', '0SoUrPPMsianzDzdN4YONFScESbRcAsJKm5MywLbsrZR1')\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#setup function\n",
    "def search_term(term, limit):\n",
    "    \n",
    "    #setup csv writing capabilities\n",
    "    #result = open(outputFile, 'a')\n",
    "    #csvWriter = csv.writer(result)\n",
    "    #csvWriter.writerow([\"Number of Followers\",\"Location\",\"Screen Name\", \"Tweet\"])\n",
    " \n",
    "    #initialize counter and empty list for storing each hit in\n",
    "    i = 0\n",
    "    number_of_followers_list = []\n",
    "    location_list = []\n",
    "    screen_name_list = []\n",
    "    tweet_text_list = []\n",
    "    \n",
    "    #Hit Twitter, loop as specified, and grab the information I want\n",
    "    for tweet in tweepy.Cursor(api.search, q = term, lang = \"en\").items(limit):\n",
    "        number_of_followers = tweet.user.followers_count\n",
    "        location = tweet.user.location\n",
    "        screen_name = tweet.user.screen_name\n",
    "        tweet_text = tweet.text.encode('utf-8')\n",
    "        \n",
    "        #repeatedly append each respective list with the information grabbed from Twitter\n",
    "        number_of_followers_list.append(number_of_followers)\n",
    "        location_list.append(location)\n",
    "        screen_name_list.append(screen_name)\n",
    "        tweet_text_list.append(tweet_text)\n",
    "        i = i+1\n",
    "        \n",
    "    #Finally, create and export dataframes using the fully filled-out lists. This whole thing is just to make three separate files\n",
    "    if g == 'Gibson':\n",
    "        gibson_data = {'Number of Followers': number_of_followers_list , 'Location': location_list,'Screen Name': screen_name_list, 'Tweet': tweet_text_list}\n",
    "        gibson_df = pd.DataFrame(data=gibson_data)\n",
    "        gibson_df.to_csv (r'C:\\Users\\Home\\Documents\\Web Analytics\\Assignments\\Assignment_2\\Wordclouds_Wordlists_and_Twitter Results\\Gibson_Python_Export.csv', index = False, header=True)\n",
    "    \n",
    "    elif g == 'Fender':\n",
    "        fender_data = {'Number of Followers': number_of_followers_list , 'Location': location_list,'Screen Name': screen_name_list, 'Tweet': tweet_text_list}\n",
    "        fender_df = pd.DataFrame(data=fender_data)\n",
    "        fender_df.to_csv (r'C:\\Users\\Home\\Documents\\Web Analytics\\Assignments\\Assignment_2\\Wordclouds_Wordlists_and_Twitter Results\\Fender_Python_Export.csv', index = False, header=True)\n",
    "    \n",
    "    elif g == 'Paul Reed Smith':\n",
    "        PRS_data = {'Number of Followers': number_of_followers_list , 'Location': location_list,'Screen Name': screen_name_list, 'Tweet': tweet_text_list}\n",
    "        PRS_df = pd.DataFrame(data=PRS_data)\n",
    "        PRS_df.to_csv (r'C:\\Users\\Home\\Documents\\Web Analytics\\Assignments\\Assignment_2\\Wordclouds_Wordlists_and_Twitter Results\\Paul_Reed_Smith_Python_Export.csv', index = False, header=True)\n",
    "        \n",
    "    print(str(i)+\" rows of data crawled from Twitter related to \" + term)\n",
    "    #result.close()\n",
    "    \n",
    "# edit the function inputs and run\n",
    "brand_list = ['Gibson','Fender', 'Paul Reed Smith']\n",
    "for g in brand_list:\n",
    "    search_term(g, 100)                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question #2: Using BeautifulSoup to crawl Yelp\n",
    "\n",
    "Restaurant name: Shanghai 21\n",
    "Starting URL: https://www.yelp.com/biz/shanghai-21-new-york-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Establish website URLs.\n",
    "URL_list = ['https://www.yelp.com/biz/shanghai-21-new-york-2',\n",
    "            'https://www.yelp.com/biz/shanghai-21-new-york-2?start=20',\n",
    "            'https://www.yelp.com/biz/shanghai-21-new-york-2?start=40',\n",
    "            'https://www.yelp.com/biz/shanghai-21-new-york-2?start=60',\n",
    "            'https://www.yelp.com/biz/shanghai-21-new-york-2?start=80']\n",
    "\n",
    "#----------Step 1: Obtain Text Reviews-------------------\n",
    "\n",
    "#initialize blank list of Yelp reviews\n",
    "review=[]\n",
    "\n",
    "#Loop over all the URLs to create a list of reviews\n",
    "\n",
    "for i in URL_list:\n",
    "# Open the url \n",
    "    ourUrl = urllib.request.urlopen(i)   \n",
    "# Create a BeautifulSoup object which represents the document as a nested data structure\n",
    "    soup = BeautifulSoup(ourUrl,'html.parser')   \n",
    "#extract and append reviews to blank list\n",
    "    for i in soup.find_all('p',{'class':'lemon--p__373c0__3Qnnj text__373c0__2Kxyz comment__373c0__3EKjH text-color--normal__373c0__3xep9 text-align--left__373c0__2XGa-'}):\n",
    "        per_review=i.find('span')  # extract review\n",
    "        review.append(per_review)  # append review\n",
    "\n",
    "# Strip the reviews of tags not relevant to the text of the review\n",
    "\n",
    "clean_review_list=[]  # create an empty list to store new reviews\n",
    "\n",
    "for i in review:\n",
    "    cleaned_review = str(i).replace('<span class=\"lemon--span__373c0__3997G raw__373c0__3rcx7\" lang=\"en\">','') #remove html tags \n",
    "    cleaned_review = cleaned_review.replace(\"<br/>\",\"\").replace(\"</span>\", \"\")\n",
    "    clean_review_list.append(cleaned_review)\n",
    "    \n",
    "#----------Step 2: Obtain Ratings-------------------\n",
    "ratings_list = []\n",
    "\n",
    "for i in URL_list:\n",
    "# Open the url \n",
    "    ourUrl = urllib.request.urlopen(i)  \n",
    "# Create a BeautifulSoup object which represents the document as a nested data structure\n",
    "    soup = BeautifulSoup(ourUrl,'html.parser')  \n",
    "#extract and append ratings to blank list\n",
    "    for i in soup.find_all('li',{'class':'lemon--li__373c0__1r9wz margin-b3__373c0__q1DuY padding-b3__373c0__342DA border--bottom__373c0__3qNtD border-color--default__373c0__3-ifU'}): \n",
    "        per_rating=str(i.find_all('span',{'class':'lemon--span__373c0__3997G display--inline__373c0__3JqBP border-color--default__373c0__3-ifU'}))\n",
    "        ratings_list.append(per_rating)\n",
    "\n",
    "#----------Step 3: Put results into a cleaned dataframe and write output to a csv file.-----------\n",
    "\n",
    "#clean the unwanted junk out of the \"ratings_list\" list\n",
    "##Note, the above find_all creates a list and then we throw that list into a list. So, when we index slice, we have to go [0][1:10], for example.\n",
    "cleaned_ratings_list = []\n",
    "for r in range(len(ratings_list)):\n",
    "    rating_string = ratings_list[r][124:137]\n",
    "    cleaned_ratings_list.append(rating_string)\n",
    "\n",
    "#make a review ID column\n",
    "reviewID = []\n",
    "for j in range(1,len(cleaned_ratings_list)+1):\n",
    "    reviewID.append(j)\n",
    "\n",
    "#Create and outsheet dataframe\n",
    "Yelp_data_1 = {'ReviewID':reviewID, 'Review Rating': cleaned_ratings_list,'Reviews':clean_review_list}\n",
    "Yelp_Dataframe_1 = pd.DataFrame(data=Yelp_data_1)\n",
    "\n",
    "\n",
    "Yelp_Dataframe_1.to_csv (r'C:\\Users\\Home\\Documents\\Web Analytics\\Assignments\\Assignment_2\\Yelp_Reviews.csv', index = False, header=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewID</th>\n",
       "      <th>Review Rating</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5 star rating</td>\n",
       "      <td>I was arguing with someone over which NYC rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5 star rating</td>\n",
       "      <td>First time back since they transformed Mott St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5 star rating</td>\n",
       "      <td>KEY COVID TAKEAWAYS:- Each server is wearing a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5 star rating</td>\n",
       "      <td>BEST SOUP DUMPLINGS IN NYC!!!! I've had my fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4 star rating</td>\n",
       "      <td>This place has yummy no-frills Shanghainese cu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewID  Review Rating                                            Reviews\n",
       "0         1  5 star rating  I was arguing with someone over which NYC rest...\n",
       "1         2  5 star rating  First time back since they transformed Mott St...\n",
       "2         3  5 star rating  KEY COVID TAKEAWAYS:- Each server is wearing a...\n",
       "3         4  5 star rating  BEST SOUP DUMPLINGS IN NYC!!!! I've had my fai...\n",
       "4         5  4 star rating  This place has yummy no-frills Shanghainese cu..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yelp_Dataframe_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
